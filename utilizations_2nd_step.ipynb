{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Second Step : Make Data into Train, Valid, Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from imageio import imwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split Data into Train, Valid, Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(batch_size, is_training=True):\n",
    "    '''\n",
    "    <ARGUMENTS>\n",
    "        # batch_size: # of samples that are going to be propagated through the network\n",
    "        # is_training: If user wants to train; is_training=True, or test; is_training=False\n",
    "    \n",
    "    <OUTPUTS>\n",
    "        # Training Data\n",
    "          ==> X_train, y_train, train_batch_num, X_valid, y_valid, valid_batch_num\n",
    "          \n",
    "        # Testing Data\n",
    "          ==> X_test, y_test, test_batch_num\n",
    "    '''\n",
    "    \n",
    "    path = join('Data', 'MNIST')\n",
    "    \n",
    "    if is_training:\n",
    "        '''\n",
    "         [Training datasets - IMAGES]\n",
    "          1. train_imgs = the training data\n",
    "          2. array_train_imgs = make train_imgs to an array\n",
    "          3. From the paper, they use 60K images for training.\n",
    "             But, array_train_imgs.reshape((len(array_train_imgs)/(28*28),28,28,1)) \n",
    "             cannot reshape array of size into shape (60000,28,28,1).\n",
    "             (Because the actual size is 60000.02040816326)\n",
    "             Therefore, I started from the array_train_imgs[16] so that I can make the shape 60000.            \n",
    "        '''\n",
    "        train_imgs = open(join(path, 'train-images-idx3-ubyte'))\n",
    "        array_train_imgs = np.fromfile(file=train_imgs, dtype=np.uint8)\n",
    "        X_training = array_train_imgs[16:].reshape(int(len(array_train_imgs[16:])/(28*28)), 28, 28, 1).astype(np.float32)\n",
    "        \n",
    "        '''\n",
    "         [Training datasets - LABELS]\n",
    "          1. train_labels = the training data's labels\n",
    "          2. array_train_labels = make train_labels to an array\n",
    "          3. From the paper, they use 60K images for training.\n",
    "             But, array_train_labels.shape = (60008,)\n",
    "             Therefore, I started from the array_train_labels[8] so that I can make the shape 60000.\n",
    "        '''\n",
    "        train_labels = open(join(path, 'train-labels-idx1-ubyte'))\n",
    "        array_train_labels = np.fromfile(file=train_labels, dtype=np.uint8)\n",
    "        y_training = array_train_labels[8:].reshape(int(len(array_train_labels[8:]))).astype(np.int32)\n",
    "\n",
    "        '''\n",
    "        [Training datasets - Training and Validation datasets]\n",
    "         - Divide training datasets to train set(55000), validation set(5000)\n",
    "         - Why should I need to divide it by 255.? Anyone?? Please help.\n",
    "        '''\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_training, y_training, test_size = 5000/60000, train_size = 55000/60000)\n",
    "        \n",
    "        X_train = X_train / 255.\n",
    "        X_valid = X_valid / 255.\n",
    "        \n",
    "        '''\n",
    "        [Set batch numbers]\n",
    "         - training batch num, validation batch num\n",
    "        '''\n",
    "        train_batch_num = len(X_train) // batch_size\n",
    "        valid_batch_num = len(X_valid) // batch_size\n",
    "\n",
    "        return X_train, y_train, train_batch_num, X_valid, y_valid, valid_batch_num\n",
    "    \n",
    "    else:\n",
    "        '''\n",
    "        [Testing datasets - IMAGES]\n",
    "         1. test_imgs = the testing data\n",
    "         2. array_test_imgs = make test_imgs to an array\n",
    "         3. From the paper, they use 10K images for testing.\n",
    "            But, array_test_imgs.reshape((len(array_test_imgs)/(28*28),28,28,1)) \n",
    "            cannot reshape array of size into shape (10000,28,28,1).\n",
    "            (Because the actual size is 10000.020408163266)\n",
    "            Therefore, I started from the array_test_imgs[16] so that I can make the shape 10000. \n",
    "        '''\n",
    "        test_imgs = open(join(path, 't10k-images-idx3-ubyte'))\n",
    "        array_test_imgs = np.fromfile(file=test_imgs, dtype=np.uint8)\n",
    "        X_test = array_test_imgs[16:].reshape(int(len(array_test_imgs[16:])/(28*28)), 28, 28, 1).astype(np.float)\n",
    "        \n",
    "        X_test = X_test / 255.\n",
    "        \n",
    "        '''\n",
    "        [Testing datasets - LABELS]\n",
    "         1. test_labels = the testing data's labels\n",
    "         2. array_test_labels = make test_labels to an array\n",
    "         3. From the paper, they use 10K images for training.\n",
    "            But, array_train_labels.shape = (10008,)\n",
    "            Therefore, I started from the array_train_labels[8] so that I can make the shape 10000.\n",
    "        '''\n",
    "        test_labels = open(join(path, 't10k-labels-idx1-ubyte'))\n",
    "        array_test_labels = np.fromfile(file=test_labels, dtype=np.uint8)\n",
    "        y_test = array_test_labels[8:].reshape((10000)).astype(np.int32)\n",
    "\n",
    "        '''\n",
    "        [Set batch numbers]\n",
    "         - testing batch num\n",
    "        '''\n",
    "        test_batch_num = len(X_test) // batch_size\n",
    "        \n",
    "        return X_test, y_test, test_batch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Make Data into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_data(batch_size, num_threads):\n",
    "    '''\n",
    "    <ARGUMENTS>\n",
    "        # batch_size: # of samples that going to be propagated through the network\n",
    "        # num_threads: The number of threads enqueuing tensor_list\n",
    "    \n",
    "    <OUTPUTS>\n",
    "        # (X, y) <-- Tensors\n",
    "        \n",
    "    [Transform Data into Tensor]\n",
    "    1. Split the data into train-set and valid-set\n",
    "    2. Produces a slice of each Tensor in tensor_list - Turn splitted data into tensor\n",
    "    3. Shuffle the data\n",
    "    '''\n",
    "    X_train, y_train, train_batch_num, X_valid, y_valid, valid_batch_num = split_data(batch_size, is_training=True)    \n",
    "    \n",
    "    data_queues = tf.train.slice_input_producer([X_train, y_train])\n",
    "    \n",
    "    X, y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
    "                                  batch_size=batch_size,\n",
    "                                  capacity=batch_size * 64,\n",
    "                                  min_after_dequeue=batch_size * 32,\n",
    "                                  allow_smaller_final_batch=False)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_imgs(imgs, size, path):\n",
    "    '''\n",
    "    <ARGUMENTS>\n",
    "        # imgs: [batch_size, image_height, image_width]\n",
    "        # size: a list with tow int elements, [image_height, image_width]\n",
    "        # path: the path to save images\n",
    "        \n",
    "    <OUPUTS>\n",
    "        # Write Merged Images as a file.\n",
    "    '''\n",
    "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
    "    return imwrite(path, merge_imgs(imgs, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Merge Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_imgs(images, size):\n",
    "    '''\n",
    "    <ARGUMENTS>\n",
    "        # images: [batch_size, image_height, image_width] ==> inverse_transform\n",
    "        # size: a list with tow int elements, [image_height, image_width]\n",
    "        \n",
    "    <OUTPUTS>\n",
    "        # Merged Images\n",
    "    '''\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
