{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Third Step: CapsLayer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) License to Huadong Liao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "License: Apache-2.0\n",
    "Author: Huadong Liao\n",
    "E-mail: naturomics.liao@gmail.com\n",
    "\n",
    "[ I changed very little. The base code is Naturomics' code. ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from configurations import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Set Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) CapsLayer (MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsLayer(object):\n",
    "    '''\n",
    "    < ARGUMENTS >\n",
    "        input: A 4-D tensor.\n",
    "        num_outputs: the number of capsule in this layer.\n",
    "        vec_len: integer, the length of the output vector of a capsule.\n",
    "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
    "            fully connected or convolution, for the future expansion capability\n",
    "        with_routing: boolean, this capsule is routing with the\n",
    "                      lower-level layer capsule.\n",
    "    < OUTPUTS >\n",
    "        A 4-D tensor.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
    "        self.num_outputs = num_outputs\n",
    "        self.vec_len = vec_len\n",
    "        self.with_routing = with_routing\n",
    "        self.layer_type = layer_type\n",
    "\n",
    "        \n",
    "    def __call__(self, input, kernel_size=None, stride=None):\n",
    "        '''\n",
    "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
    "        '''\n",
    "        if self.layer_type == 'CONV':\n",
    "            self.kernel_size = kernel_size\n",
    "            self.stride = stride\n",
    "\n",
    "            if not self.with_routing:\n",
    "                '''\n",
    "                [ PrimaryCaps layer ]\n",
    "                 - a convolutional layer\n",
    "                     # input: [batch_size, 20, 20, 256]\n",
    "                '''\n",
    "                assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
    "\n",
    "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
    "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
    "                                                    activation_fn=tf.nn.relu)\n",
    "\n",
    "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
    "\n",
    "                '''\n",
    "                [ Shape ]\n",
    "                    # [batch_size, 1152, 8, 1\n",
    "                '''\n",
    "                capsules = squash(capsules)\n",
    "                \n",
    "                assert capsules.get_shape() == [cfg.batch_size, 1152, 8, 1]\n",
    "                \n",
    "                return capsules\n",
    "\n",
    "            \n",
    "        if self.layer_type == 'FC':\n",
    "            if self.with_routing:\n",
    "                '''\n",
    "                [ DigitCaps layer ]\n",
    "                 - a fully connected layer\n",
    "                    # Reshape the input shapel into [batch_size, 1152, 1, 8, 1]\n",
    "                '''\n",
    "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
    "\n",
    "                with tf.variable_scope('routing') as scope:\n",
    "                    '''\n",
    "                    b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
    "                          about the reason of using 'batch_size', see issue #21\n",
    "                    '''\n",
    "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
    "                    capsules = routing(self.input, b_IJ)\n",
    "                    capsules = tf.squeeze(capsules, axis=1)\n",
    "\n",
    "            return capsules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routing(input, b_IJ):\n",
    "    ''' The routing algorithm.\n",
    "    < ARGUMENTS >\n",
    "        < INPUTS > \n",
    "               A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
    "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
    "        \n",
    "        < OUTPUTS >\n",
    "                A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
    "                representing the vector output `v_j` in the layer l+1\n",
    "        \n",
    "        Notes:\n",
    "               u_i represents the vector output of capsule i in the layer l, and\n",
    "               v_j the vector output of capsule j in the layer l+1.\n",
    "     '''\n",
    "\n",
    "    '''\n",
    "     # W: [num_caps_i, num_caps_j, len_u_i, len_v_j]\n",
    "    '''\n",
    "    with tf.name_scope('weight') as scope:\n",
    "        W = tf.get_variable('weight', shape=(1, 1152, 10, 8, 16), dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "    '''\n",
    "    [ Equation ] \n",
    "     # Calculate u_hat\n",
    "       - Do tiling for input and W before matmul\n",
    "       - # input ==> [batch_size, 1152, 10, 8, 1]\n",
    "       - # W ==> [batch_size, 1152, 10, 8, 16]\n",
    "    '''\n",
    "    input = tf.tile(input, [1, 1, 10, 1, 1])\n",
    "    W = tf.tile(W, [cfg.batch_size, 1, 1, 1, 1])\n",
    "    \n",
    "    assert input.get_shape() == [cfg.batch_size, 1152, 10, 8, 1]\n",
    "    \n",
    "    '''\n",
    "    [ Last 2 dimensions ]\n",
    "     - # [8, 16].T x [8, 1] => [16, 1] => [batch_size, 1152, 10, 16, 1]\n",
    "     - # tf.scan, 3 iter, 1080ti, 128 batch size: 10min/epoch\n",
    "     - # u_hat = tf.scan(lambda ac, x: tf.matmul(W, x, transpose_a=True), input, initializer=tf.zeros([1152, 10, 16, 1]))\n",
    "     - # tf.tile, 3 iter, 1080ti, 128 batch size: 6min/epoch   \n",
    "    '''\n",
    "    u_hat = tf.matmul(W, input, transpose_a=True)\n",
    "    \n",
    "    assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "    '''\n",
    "    [ u_hat_stopped ] \n",
    "     - # u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
    "    '''\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
    "\n",
    "    for r_iter in range(cfg.iter_routing):\n",
    "        with tf.variable_scope('iter_' + str(r_iter)):\n",
    "            \n",
    "            c_IJ = tf.nn.softmax(b_IJ, dim=2)\n",
    "            \n",
    "            '''\n",
    "            [ Last Iteration ]\n",
    "            '''\n",
    "            if r_iter == cfg.iter_routing - 1:\n",
    "                '''\n",
    "                    # weighting u_hat with c_IJ, element-wise in the last two dims\n",
    "                    # => [batch_size, 1152, 10, 16, 1]\n",
    "                '''\n",
    "                s_J = tf.multiply(c_IJ, u_hat)\n",
    "                '''\n",
    "                    # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
    "                '''\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n",
    "                \n",
    "                assert s_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
    "\n",
    "                '''\n",
    "                [ Squashing ]\n",
    "                '''\n",
    "                v_J = squash(s_J)\n",
    "                \n",
    "                assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
    "                \n",
    "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
    "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "                '''\n",
    "                    # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
    "                    # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
    "                    # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
    "                '''\n",
    "                v_J_tiled = tf.tile(v_J, [1, 1152, 1, 1, 1])\n",
    "                u_produce_v = tf.matmul(u_hat_stopped, v_J_tiled, transpose_a=True)\n",
    "                \n",
    "                assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "                b_IJ += u_produce_v\n",
    "\n",
    "    return v_J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Squasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(vector):\n",
    "    '''Squashing function corresponding to Eq. 1\n",
    "    < ARGUMENTS >\n",
    "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
    "    < OUTPUTS >\n",
    "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
    "    '''\n",
    "    vec_squared_norm = tf.reduce_sum(tf.square(vector), -2, keep_dims=True)\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
    "    vec_squashed = scalar_factor * vector  # element-wise\n",
    "    \n",
    "    return vec_squashed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
