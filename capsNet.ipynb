{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fourth Step: CapsNet (MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "License: Apache-2.0\n",
    "Author: Huadong Liao\n",
    "E-mail: naturomics.liao@gmail.com\n",
    "\n",
    "[ I changed very little. The base code is Naturomics' code. ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from configurations import cfg\n",
    "from utilizations import tensor_data\n",
    "from capsLayer import CapsLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Set epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) CapsNet (MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(object):\n",
    "    '''\n",
    "    [ CapsuleNet's Main Architecture ]\n",
    "    '''\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        '''\n",
    "        [ Make a Graph ]\n",
    "        '''\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            if is_training: # When Training\n",
    "                '''\n",
    "                [ Get Tensor Data to build Architecture ] <-- ** Main Part **\n",
    "                '''\n",
    "                self.X, self.labels = tensor_data(cfg.batch_size, cfg.num_threads)\n",
    "                self.y = tf.one_hot(self.labels, depth=10, axis=1, dtype=tf.float32)\n",
    "\n",
    "                self.build_architecture()\n",
    "                self.loss()\n",
    "                self._summary()\n",
    "\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer()\n",
    "                self.train_operation = self.optimizer.minimize(self.total_loss, global_step=self.global_step) \n",
    "                \n",
    "            else: # When Testing\n",
    "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, 28, 28, 1))\n",
    "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
    "                self.y = tf.reshape(self.labels, shape=(cfg.batch_size, 10, 1))\n",
    "                self.build_architecture()\n",
    "\n",
    "        tf.logging.info('Setting up the main structure')\n",
    "\n",
    "        \n",
    "    def build_architecture(self):\n",
    "        \n",
    "        '''\n",
    "        [ 1st Convolution Layer ]\n",
    "            # Conv1, [batch_size, 20, 20, 256]\n",
    "        '''\n",
    "        with tf.variable_scope('Conv1_layer'):\n",
    "            \n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
    "                                             kernel_size=9, stride=1,\n",
    "                                             padding='VALID')\n",
    "            \n",
    "            assert conv1.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
    "\n",
    "        '''\n",
    "        [ Primary Capsules Layer]\n",
    "            # PCL, [batch_size, 1152, 8, 1]\n",
    "        '''            \n",
    "        with tf.variable_scope('PrimaryCaps_layer'):\n",
    "            \n",
    "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
    "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
    "            \n",
    "            assert caps1.get_shape() == [cfg.batch_size, 1152, 8, 1]\n",
    "\n",
    "        '''\n",
    "        [ DigitCaps Layer]\n",
    "            # DGL, [batch_size, 10, 16, 1]\n",
    "        '''             \n",
    "        with tf.variable_scope('DigitCaps_layer'):\n",
    "            digitCaps = CapsLayer(num_outputs=10, vec_len=16, with_routing=True, layer_type='FC')\n",
    "            self.caps2 = digitCaps(caps1)\n",
    "\n",
    "        '''\n",
    "        [ Masking ]\n",
    "         1. calc ||v_c||, then do softmax(||v_c||) ==> [batch_size, 10, 16, 1] ==> [batch_size, 10, 1, 1]\n",
    "         2. pick out the index of max softmax val of the 10 caps ==> [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "         3. indexing\n",
    "         4. masking with true label <-- default mode   \n",
    "        '''\n",
    "        with tf.variable_scope('Masking') as scope:\n",
    "            # 1\n",
    "            self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)\n",
    "            self.softmax_v = tf.nn.softmax(self.v_length, dim=1)\n",
    "            assert self.softmax_v.get_shape() == [cfg.batch_size, 10, 1, 1]\n",
    "\n",
    "            # 2\n",
    "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
    "            assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
    "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
    "\n",
    "            # 3\n",
    "            if not cfg.mask_with_y:\n",
    "                masked_v = []\n",
    "                for batch_size in range(cfg.batch_size):\n",
    "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
    "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "\n",
    "                self.masked_v = tf.concat(masked_v, axis=0)\n",
    "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
    "            # 4\n",
    "            else:\n",
    "                # self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.y, (-1, 10, 1)), transpose_a=True)\n",
    "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.y, (-1, 10, 1)))\n",
    "                self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)\n",
    "\n",
    "        '''\n",
    "        [ Reconstructe the MNIST images with 3 FC layers ]\n",
    "            # 1st FC: [batch_size, 1, 16, 1] \n",
    "              ==> 2nd FC: [batch_size, 16] \n",
    "              ==> 3rd FC: [batch_size, 512]\n",
    "        '''       \n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            # 1st\n",
    "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
    "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "            assert fc1.get_shape() == [cfg.batch_size, 512]\n",
    "            \n",
    "            # 2nd\n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "            assert fc2.get_shape() == [cfg.batch_size, 1024]\n",
    "            \n",
    "            # 3rd\n",
    "            self.decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        [The Margin Loss] ==> [batch_size, 10, 1, 1]\n",
    "         1. max_l = max(0, m_plus-||v_c||)^2\n",
    "         2. max_r = max(0, ||v_c||-m_minus)^2\n",
    "         3. reshape: [batch_size, 10, 1, 1] ==> [batch_size, 10]\n",
    "         4. calculate L_c\n",
    "         5. calculate margin_loss\n",
    "        '''\n",
    "        \n",
    "        # 1\n",
    "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
    "        \n",
    "        # 2\n",
    "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
    "        assert max_l.get_shape() == [cfg.batch_size, 10, 1, 1]\n",
    "        \n",
    "        # 3\n",
    "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
    "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
    "\n",
    "        # 4\n",
    "        T_c = self.y\n",
    "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
    "        \n",
    "        # 5 \n",
    "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "\n",
    "        '''\n",
    "        [The Reconstruction Loss]\n",
    "         1. orgin = Reshape X\n",
    "         2. squared = (decoded - orgin)^2\n",
    "         3. reconstruction_error = sum(squared)\n",
    "        '''\n",
    "        # 1\n",
    "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
    "        \n",
    "        # 2\n",
    "        squared = tf.square(self.decoded - orgin)\n",
    "        \n",
    "        # 3\n",
    "        self.reconstruction_error = tf.reduce_mean(squared)\n",
    "\n",
    "        '''\n",
    "        [ Total Loss ]\n",
    "            # The paper uses sum of squared error as reconstruction error, but we\n",
    "            # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
    "            # mean squared error. In order to keep in line with the paper,the\n",
    "            # regularization scale should be 0.0005*784=0.392\n",
    "        '''\n",
    "        # 3. Total loss\n",
    "\n",
    "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_error\n",
    "\n",
    "    '''\n",
    "    [ Summary ]\n",
    "    '''\n",
    "    def _summary(self):\n",
    "        train_summary = []\n",
    "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
    "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_error))\n",
    "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
    "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, 28, 28, 1))\n",
    "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
    "        self.train_summary = tf.summary.merge(train_summary)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
